---
title: Hadoop学习之2014.11.25
tags: [Hadoop]
category: 技术探索
date: 2014-11-25 18:32:36
---

为了使用hadoop2.5.1,花费了n多时间，最后还是改用了hadoop1.2.1

原因在于，似乎没有hadoop2.x的中文文档（囧..），只找到了一个1.0.4的doc，hadoop2.x和hadoop1.x的差别很大，貌似还是由不同的组织进行维护的，发现1.0.4的wordcount V2.0在安装2.5.1不能运行后，果断安装了一个hadoop1.2.1..

安装1.2.1用的是rpm包

不同于直接解压.tar.gz，用rpm安装的话，hadoop的一些文件的位置是变化了的，譬如，启动关闭dfs或者mapreduce的.sh脚本默认放在了/usr/sbin目录下，hadoop的配置文件如hadoop-env.xml、hdfs-sites.xml的都放在了/etc/hadoop目录下，而hadoop安装在了/usr/share/hadoop下

安装并配置完毕后，开始运行wordcount，出现了“There is insufficient memory for the Java Runtime Environment to continue”这个错误，调试了很久，最后还是含泪加了我的阿里云ECS的内存，从1G升级到2G，重启，果断那个错误消失了..

一阵学习中..

看了http://hadoop.apache.org/docs/r1.0.4/cn/cluster_setup.html后，突然想了解下hadoop的启动过程..

从start-dfs.sh到hadoop-daemons.sh再到slaves.sh，发现了

ssh $HADOOP_SSH_OPTS $slave $"${@// /\\ }" 2&gt;&amp;1 | sed "s/^/$slave: /" &amp;

这句，看来应该是nameNode主机通过ssh远程在dataNode主机执行命令完成启动dataNode的hadoop的..

对于linux的sed的使用不是很熟悉，想以后该学习下..

但$"${@// /\\ }"这句，我始终不太懂..特别是类似${var/ /}的用法..

google了一下，详见另一篇文件&lt;shell学习之2014.11.25&gt;..